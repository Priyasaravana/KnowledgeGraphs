{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge graph embedding model \n",
    "\n",
    "- ComplEx from torch_geometric.nn.kge for the Knowledge Graph Embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EzhilPriyadharshiniK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.nn.kge import ComplEx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is a DataFrame `df` with 'head', 'relation', 'tail' columns\n",
    "# Load data, Reads the data from a CSV file containing knowledge graph facts for books.\n",
    "# The dataset should contain 'head', 'relation', and 'tail' columns representing entities and relationships in the graph.\n",
    "df = pd.read_csv('data/books_graph_facts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model - Complex model from torch.geometric. Complex models relations as complex-valued bilinear mappings between head and tail entities using the Hermetian dot product.  \n",
    "    <img src=\"./resources/ComplexModel.png\" alt=\"Complex\" width=\"300\"/>  \n",
    "- Optimizer - To update the model parameters based on the computed gradients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: DataLoader,\n",
    "    model: ComplEx,\n",
    "    optimizer: Optimizer, \n",
    "    device: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    General training loop for PyTorch models.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader object for batching the training data.\n",
    "        model (ComplEx): The ComplEx model instance used for link prediction.\n",
    "        optimizer (Optimizer): Optimizer for updating model weights.\n",
    "        device (str): Device to run the training on, either 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function iterates over the DataLoader batches and performs the following:\n",
    "    - Moves the data to the specified device (CPU or GPU).\n",
    "    - Calculates the loss using the ComplEx model's loss function.\n",
    "    - Performs backpropagation and optimization to minimize the loss.\n",
    "    - Outputs the loss at specified intervals for monitoring.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    print('###############Training Starts here#########################', size)\n",
    "    \n",
    "    model.train() \n",
    "    total_loss = total_examples = 0\n",
    "    for batch, (head_index, rel_type, tail_index) in enumerate(dataloader):\n",
    "        # Move head, relation, and tail tensors to device\n",
    "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
    "\n",
    "        # Calculate the loss using ComplEx's loss function\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad() # to clears previous gradients and avoids the gradient accumulation\n",
    "        loss.backward() # calculate gradients for each model parameters based on the loss\n",
    "        optimizer.step() # adjust the model weights\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "        if batch % 1000 == 0:\n",
    "            loss_val = loss.item()\n",
    "            current = batch * len(head_index)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the testing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs the evaluation on the model with the test data and calculates the below metrics,    \n",
    "- Rank: A measure of how close the predictions are to the correct entity in sorted order.\n",
    "- MRR (Mean Reciprocal Rank): The average of the reciprocal ranks of the predicted results, providing a single-score summary.\n",
    "- Hits@k: Checks if the correct tail entity is within the top k predictions (here k=10). Hits@10 evaluates the number of correct predictions within the top 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(\n",
    "    dataloader: DataLoader,\n",
    "    model: ComplEx,\n",
    "    device: str,\n",
    "    k: int = 10 \n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Test method for ComplEx model.\n",
    "    Evaluates the model using Hits@k and MRR metrics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize lists to collect the full test set\n",
    "    all_head_index = []\n",
    "    all_rel_type = []\n",
    "    all_tail_index = []\n",
    "\n",
    "    # Loop over the DataLoader and collect all test data\n",
    "    for batch, (head_index, rel_type, tail_index) in enumerate(dataloader):\n",
    "        # Move head, relation, and tail tensors to device\n",
    "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
    "        \n",
    "        # Append each batch to the list\n",
    "        all_head_index.append(head_index)\n",
    "        all_rel_type.append(rel_type)\n",
    "        all_tail_index.append(tail_index)\n",
    "\n",
    "    # Concatenate all batches into a single tensor for each\n",
    "    all_head_index = torch.cat(all_head_index, dim=0)\n",
    "    all_rel_type = torch.cat(all_rel_type, dim=0)\n",
    "    all_tail_index = torch.cat(all_tail_index, dim=0)\n",
    "\n",
    "    # Call the built-in `test()` method from PyTorch Geometric's ComplEx model\n",
    "    rank, mrr, hits_at_10 = model.test(\n",
    "        head_index=all_head_index,\n",
    "        rel_type=all_rel_type,\n",
    "        tail_index=all_tail_index,\n",
    "        batch_size=2000,\n",
    "        k=10\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f'Validation Mean Rank: {rank:.2f}, Validation MRR: {mrr:.4f}, '\n",
    "      f'Validation Hits@10: {hits_at_10:.4f}')\n",
    "\n",
    "    return rank, mrr, hits_at_10\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data - tokenization, train test and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique mappings for entities (from head and tail) and relations\n",
    "entities = pd.concat([df['head'], df['tail']]).unique()  # Combine head and tail to get all unique entities\n",
    "relations = df['relation'].unique()  # Get unique relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings to convert entities and relations into integer indices\n",
    "entity_to_id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation_to_id = {relation: idx for idx, relation in enumerate(relations)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mappings to create new 'head_id', 'relation_id', and 'tail_id' columns\n",
    "df['head_id'] = df['head'].map(entity_to_id)\n",
    "df['relation_id'] = df['relation'].map(relation_to_id)\n",
    "df['tail_id'] = df['tail'].map(entity_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is stored in a pandas DataFrame 'df' with 'head', 'relation', 'tail' columns\n",
    "# and has been mapped to integer indices\n",
    "\n",
    "# Create the tensors for head, relation, and tail indices\n",
    "head_index = torch.tensor(df['head_id'].values, dtype=torch.long)\n",
    "rel_type = torch.tensor(df['relation_id'].values, dtype=torch.long)\n",
    "tail_index = torch.tensor(df['tail_id'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the data into training and temp sets (80% train, 20% for validation and test)\n",
    "train_head, temp_head, train_rel, temp_rel, train_tail, temp_tail = train_test_split(\n",
    "    head_index, rel_type, tail_index, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Split the temp_data into validation and test sets (50% validation, 50% test)\n",
    "val_head, test_head, val_rel, test_rel, val_tail, test_tail = train_test_split(\n",
    "    temp_head, temp_rel, temp_tail, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataloader and setting the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TensorDatasets for each split\n",
    "train_dataset = TensorDataset(train_head, train_rel, train_tail)\n",
    "val_dataset = TensorDataset(val_head, val_rel, val_tail)\n",
    "test_dataset = TensorDataset(test_head, test_rel, test_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 480000\n",
      "Validation set size: 60000\n",
      "Test set size: 60000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "# Check the sizes of the splits\n",
    "print(f\"Training set size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test set size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the ComplEx model\n",
    "num_nodes = len(pd.concat([df['head'], df['tail']]).unique())  # Total number of unique entities (nodes)\n",
    "num_relations = len(df['relation'].unique())  # Total number of unique relations\n",
    "hidden_channels = 100  # Embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "# Initialize ComplEx model\n",
    "model = ComplEx(num_nodes=num_nodes, num_relations=num_relations, hidden_channels=hidden_channels).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model, hyperparameter evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Training Starts here######################### 480000\n",
      "loss: 0.245957  [    0/480000]\n",
      "loss: 0.154188  [64000/480000]\n",
      "loss: 0.153189  [128000/480000]\n",
      "loss: 0.286057  [192000/480000]\n",
      "loss: 0.274347  [256000/480000]\n",
      "loss: 0.337183  [320000/480000]\n",
      "loss: 0.252536  [384000/480000]\n",
      "loss: 0.237739  [448000/480000]\n",
      "Epoch: 001, Loss: 0.2179\n",
      "###############Training Starts here######################### 480000\n",
      "loss: 0.258786  [    0/480000]\n",
      "loss: 0.167268  [64000/480000]\n",
      "loss: 0.230701  [128000/480000]\n",
      "loss: 0.243550  [192000/480000]\n",
      "loss: 0.298122  [256000/480000]\n",
      "loss: 0.286704  [320000/480000]\n",
      "loss: 0.184250  [384000/480000]\n",
      "loss: 0.249438  [448000/480000]\n",
      "Epoch: 002, Loss: 0.2054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [04:50<00:00, 206.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Rank: 1894.23, Validation MRR: 0.0322, Validation Hits@10: 0.0803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [03:11<00:00, 313.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Rank: 1886.08, Validation MRR: 0.0327, Validation Hits@10: 0.0818\n",
      "Test Mean Rank: 1886.08, Test MRR: 0.0327, Test Hits@10: 0.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    loss = train(train_loader, model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    if epoch % 2 == 0:\n",
    "        rank, mrr, hits = test(val_loader, model, 'cuda')\n",
    "        \n",
    "rank, mrr, hits_at_10 = test(test_loader, model, 'cuda')\n",
    "print(f'Test Mean Rank: {rank:.2f}, Test MRR: {mrr:.4f}, '\n",
    "      f'Test Hits@10: {hits_at_10:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning:\n",
    "Model needs the hyperparameter tuning to improve ranking performance.   \n",
    "The model is making progress but might benefit from further optimization. Here are a few suggestions based on this training and evaluation output:\n",
    "\n",
    "- Optimizer - trying the different optimizer like Adagrad, Adam, SGD will be helpful to tune the model for more performace.\n",
    "- Early stopping - stops the model training if the model performance was not improving for some epochs.\n",
    "- Learning Rate Adjustment: Lowering the learning rate or using a learning rate scheduler might help stabilize the training loss.\n",
    "- Model Complexity: A more complex or fine-tuned model could potentially improve MRR and Hits@10.\n",
    "- Data Augmentation: If possible, enhancing the dataset or applying regularization methods could help the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_model(model, optimizer, epoch, file_path=\"model/complex_model.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the model and optimizer state dictionaries to a file for later use.\n",
    "    \"\"\"\n",
    "    # Save the model's state dict and optimizer state dict\n",
    "    torch.save({\n",
    "        'epoch': epoch,  # Save the current epoch number\n",
    "        'model_state_dict': model.state_dict(),  # Model parameters\n",
    "        'optimizer_state_dict': optimizer.state_dict(),  # Optimizer parameters\n",
    "    }, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage after training the model for an epoch\n",
    "save_model(model, optimizer, epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model against downstream task like link predictions, classsification will help to see the model performance on potential applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
